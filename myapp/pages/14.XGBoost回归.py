import streamlit as st
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import numpy as np

st.set_page_config(page_title="XGBoost å›å½’åˆ†æ", layout="wide")
st.title("ğŸ“ˆ XGBoost å›å½’å»ºæ¨¡ä¸åˆ†æ")

# æ•°æ®ä¸Šä¼ 
uploaded_file = st.sidebar.file_uploader("ğŸ“‚ ä¸Šä¼ æ•°æ®æ–‡ä»¶ (CSV æ ¼å¼)", type=["csv"])

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.subheader("ğŸ§¾ æ•°æ®é¢„è§ˆ")
    st.dataframe(df.head(10))

    with st.sidebar:
        st.markdown("### ğŸ”§ æ¨¡å‹å‚æ•°è®¾ç½®")
        target_col = st.selectbox("ğŸ¯ é€‰æ‹©ç›®æ ‡å˜é‡ï¼ˆYï¼‰", df.columns)
        feature_cols = st.multiselect("ğŸ“Š é€‰æ‹©ç‰¹å¾å˜é‡ï¼ˆXï¼‰", [col for col in df.columns if col != target_col])
        test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.5, 0.2, 0.05)
        max_depth = st.slider("max_depth", 1, 10, 4)
        n_estimators = st.slider("n_estimators", 10, 500, 100, step=10)
        learning_rate = st.slider("learning_rate", 0.01, 0.5, 0.1, 0.01)

    if feature_cols:
        X = df[feature_cols]
        y = df[target_col]

        # åˆ’åˆ†æ•°æ®é›†
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

        # æ¨¡å‹è®­ç»ƒ
        model = xgb.XGBRegressor(
            max_depth=max_depth,
            n_estimators=n_estimators,
            learning_rate=learning_rate,
            objective='reg:squarederror',
            random_state=42
        )
        model.fit(X_train, y_train)

        # æ¨¡å‹é¢„æµ‹
        y_pred = model.predict(X_test)

        # æ¨¡å‹è¯„ä¼°
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        r2 = r2_score(y_test, y_pred)

        st.subheader("ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ")
        st.markdown(f"**RMSEï¼ˆå‡æ–¹æ ¹è¯¯å·®ï¼‰**: `{rmse:.4f}`")
        st.markdown(f"**RÂ²ï¼ˆæ‹Ÿåˆä¼˜åº¦ï¼‰**: `{r2:.4f}`")

        # å¯è§†åŒ–ï¼šé¢„æµ‹ vs å®é™…
        st.subheader("ğŸ“‰ å®é™…å€¼ vs é¢„æµ‹å€¼")
        fig, ax = plt.subplots()
        ax.scatter(y_test, y_pred, color='dodgerblue', alpha=0.7)
        ax.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
        ax.set_xlabel("å®é™…å€¼")
        ax.set_ylabel("é¢„æµ‹å€¼")
        ax.set_title("é¢„æµ‹ç»“æœå¯¹æ¯”")
        st.pyplot(fig)

        # ç‰¹å¾é‡è¦æ€§
        st.subheader("â­ ç‰¹å¾é‡è¦æ€§")
        importance_df = pd.DataFrame({
            'feature': feature_cols,
            'importance': model.feature_importances_
        }).sort_values(by='importance', ascending=False)

        st.dataframe(importance_df)

        fig2, ax2 = plt.subplots()
        ax2.barh(importance_df['feature'], importance_df['importance'], color='skyblue')
        ax2.invert_yaxis()
        ax2.set_title("ç‰¹å¾é‡è¦æ€§å›¾")
        st.pyplot(fig2)
    else:
        st.warning("âš ï¸ è¯·å…ˆé€‰æ‹©ç‰¹å¾å˜é‡ã€‚")
else:
    st.info("ğŸ“¥ è¯·ä¸Šä¼ ä¸€ä¸ªåŒ…å«æ•°å€¼ç‰¹å¾å’Œç›®æ ‡åˆ—çš„CSVæ–‡ä»¶ã€‚")
